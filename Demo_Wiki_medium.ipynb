{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import LSHTCDataset\n",
    "from embedding import Doc2Vec\n",
    "from assemble_classifier import AssembleNoLabel, AssemblePredicted\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_name = \"wiki_mediam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-01a4f35a006e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSHTCDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset_validate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSHTCDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSHTCDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/SeniorKai/data/LSHTCDataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_name, mode, state, least_data)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleast_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         super(LSHTCDataset, self).__init__(\n\u001b[0;32m---> 14\u001b[0;31m             data_name, fold_number=0, mode=mode, state=state, sequence=False)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_datas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/SeniorKai/data/Dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_name, fold_number, mode, state, sequence)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_datas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# sparse data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/SeniorKai/data/LSHTCDataset.py\u001b[0m in \u001b[0;36mload_datas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m         if not os.path.isfile(\"data/%s/pickle/data.pickle.%s\" %\n\u001b[1;32m     22\u001b[0m                               (self.data_name, self.mode)):\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_data_lshtc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleast_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleast_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         self.datas, self.labels = prep.load_data_in_pickle(\n\u001b[1;32m     25\u001b[0m             \"%s/pickle/data.pickle.%s\" % (self.data_name, self.mode))\n",
      "\u001b[0;32m~/Work/SeniorKai/data/preparation.py\u001b[0m in \u001b[0;36msplit_data_lshtc\u001b[0;34m(data_name, least_data)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mhierarchy_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s/hierarchy.pickle\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     train_data, validate_data, train_target, validate_target = split_validate(\n\u001b[0;32m--> 157\u001b[0;31m         datas, labels, hierarchy_file_name, least_data)\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/%s/pickle/data.pickle.train'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/SeniorKai/data/preparation.py\u001b[0m in \u001b[0;36msplit_validate\u001b[0;34m(datas, labels, hierarchy_name, least_data)\u001b[0m\n\u001b[1;32m    170\u001b[0m     train_index, validate_index, cutoff_label = create_train_validate_index(\n\u001b[1;32m    171\u001b[0m         labels, hierarchy_name, least_data)\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mremap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_new_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhierarchy_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mdatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremap_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     train_data, train_target = datas[list(\n",
      "\u001b[0;32m~/Work/SeniorKai/data/hierarchy.py\u001b[0m in \u001b[0;36msave_new_hierarchy\u001b[0;34m(hierarchy_name, cutoff_label)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_new_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhierarchy_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     hierarchy, parent_of, all_name, name_to_index, level = reindex_hierarchy(\n\u001b[0;32m--> 218\u001b[0;31m         hierarchy_name)\n\u001b[0m\u001b[1;32m    219\u001b[0m     hierarchy, parent_of, all_name, name_to_index, level, remap = cutoff_label(\n\u001b[1;32m    220\u001b[0m         cutoff_label, hierarchy, parent_of, all_name, name_to_index, level)\n",
      "\u001b[0;32m~/Work/SeniorKai/data/hierarchy.py\u001b[0m in \u001b[0;36mreindex_hierarchy\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreindex_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     hierarchy, parent_of, all_name, name_to_index = create_hierarchy_structure(\n\u001b[0;32m---> 94\u001b[0;31m         file_name)\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhierarchy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_of\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mremap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/SeniorKai/data/hierarchy.py\u001b[0m in \u001b[0;36mcreate_hierarchy_structure\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtemp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "dataset_train = LSHTCDataset(data_name, \"train\")\n",
    "dataset_validate = LSHTCDataset(data_name, \"validate\")\n",
    "dataset_test = LSHTCDataset(data_name, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(dataset_train.number_of_level()):\n",
    "    print(dataset_train.check_each_number_of_class(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "aa = []\n",
    "dd = []\n",
    "for i in range(11):\n",
    "    start, end = dataset_test.level[i], dataset_test.level[i + 1]\n",
    "    train = dataset_train.number_of_data_in_each_class()[start:end]\n",
    "    validate = dataset_validate.number_of_data_in_each_class()[start:end]\n",
    "    test = dataset_test.number_of_data_in_each_class()[start:end]\n",
    "    sum_all = np.array(train) + np.array(validate) + np.array(test)\n",
    "    zero_train = np.array(train) == 0\n",
    "    zero_validate = np.array(validate) == 0\n",
    "    zero_test = np.array(test) == 0\n",
    "    zero_all = np.array(sum_all) <= 1\n",
    "    zero_inverse_all = np.array(sum_all) > 1\n",
    "    \n",
    "#     print(i)\n",
    "#     print(zero_train[zero_train & zero_validate].shape[0])\n",
    "    print(zero_train[zero_all].shape[0])\n",
    "    aa.append(zero_train[zero_all].shape[0])\n",
    "    dd.append(zero_train[zero_inverse_all].shape[0])\n",
    "#     print()\n",
    "print(sum(aa))\n",
    "print(sum(dd))\n",
    "print(sum(aa) + sum(dd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in range(11):\n",
    "    start, end = dataset_test.level[i], dataset_test.level[i + 1]\n",
    "    train = dataset_train.number_of_data_in_each_class()[start:end]\n",
    "    test = dataset_test.number_of_data_in_each_class()[start:end]\n",
    "    zero_train = np.array(train) < 20\n",
    "    zero_test = np.array(test) < 1\n",
    "    print(zero_train[zero_train].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc2vec = Doc2Vec(dataset_train.number_of_classes(), size=150, epoch=250)\n",
    "# doc2vec.fit(dataset_train.datas, dataset_train.labels, dataset_validate.datas, dataset_validate.labels)\n",
    "doc2vec.load_model(\"best_now/doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train.change_to_Doc2Vec(doc2vec)\n",
    "# dataset_validate.change_to_Doc2Vec(doc2vec)\n",
    "# dataset_test.change_to_Doc2Vec(doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = AssemblePredicted(data_name, dataset_train, dataset_validate, dataset_test, iteration=2000, batch_size=5000, hidden_size=[500,1500,2500,3500,3500,4000,4500,1500,1500,1500,1500,1500], target_hidden_size=[60,80,80,120,150,180,60,60,60,60,60], use_dropout=True, stopping_time=200, start_level=4, end_level=10000)\n",
    "# model = AssembleNoLabel(data_name, dataset_train, dataset_validate, dataset_test, iteration=2000, batch_size=5000, hidden_size=[300,1500,1500,1500,1500,1500,1500,1500,1500,1500,1500,1500], use_dropout=False, start_level=2, end_level=2, stopping_time=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.tuning_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_macro, f1_micro, f1_each = model.evaluate(\"train\", correction=False)\n",
    "print(\"F1 macro: %.4f F1 micro: %.4f\" % (f1_macro, f1_micro))\n",
    "for level, (macro, micro) in enumerate(f1_each):\n",
    "    print(\"Level: %d F1 macro: %.4f F1 micro: %.4f\" % (level, macro, micro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1_macro, f1_micro, f1_each = model.evaluate(\"test\", correction=False)\n",
    "print(\"F1 macro: %.4f F1 micro: %.4f\" % (f1_macro, f1_micro))\n",
    "for level, (macro, micro) in enumerate(f1_each):\n",
    "    print(\"Level: %d F1 macro: %.4f F1 micro: %.4f\" % (level, macro, micro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
