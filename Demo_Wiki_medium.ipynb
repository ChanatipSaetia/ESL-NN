{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data import LSHTCDataset\n",
    "from embedding import Doc2Vec\n",
    "from assemble_classifier import AssembleNoLabel, AssemblePredicted\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_name = \"wiki_mediam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Work/HMC/data/LSHTCDataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_name, mode, state, least_data)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleast_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         super(LSHTCDataset, self).__init__(\n\u001b[0;32m---> 14\u001b[0;31m             data_name, fold_number=0, mode=mode, state=state, sequence=False)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_datas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/HMC/data/Dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_name, fold_number, mode, state, sequence)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_datas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# sparse data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/HMC/data/Dataset.py\u001b[0m in \u001b[0;36mload_hierarchy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \"%s/hierarchy.pickle\" % self.data_name)\n\u001b[1;32m     35\u001b[0m         self.not_leaf_node = np.array([i in list(\n\u001b[0;32m---> 36\u001b[0;31m             self.hierarchy.keys()) for i in range(self.number_of_classes())])\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# self.leaf_node = {}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/HMC/data/Dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \"%s/hierarchy.pickle\" % self.data_name)\n\u001b[1;32m     35\u001b[0m         self.not_leaf_node = np.array([i in list(\n\u001b[0;32m---> 36\u001b[0;31m             self.hierarchy.keys()) for i in range(self.number_of_classes())])\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# self.leaf_node = {}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_train = LSHTCDataset(data_name, \"train\", least_data=2, state=\"embedding\")\n",
    "dataset_validate = LSHTCDataset(data_name, \"validate\", least_data=2, state=\"embedding\")\n",
    "# dataset_validate = 0\n",
    "dataset_test = LSHTCDataset(data_name, \"test\", least_data=2, state=\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 65, 833, 5287, 16515, 32056, 42111, 45490, 45962, 45989, 45991]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# doc2vec = Doc2Vec(dataset_train.number_of_classes(), size=150, epoch=250)\n",
    "# # doc2vec.fit(dataset_train.datas, dataset_train.labels, dataset_validate.datas, dataset_validate.labels)\n",
    "# doc2vec.load_model(\"best_now/doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doc2vec.fit(dataset_train.datas, dataset_train.labels, dataset_validate.datas, dataset_validate.labels, second_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dataset_train.change_to_Doc2Vec(doc2vec)\n",
    "# dataset_validate.change_to_Doc2Vec(doc2vec)\n",
    "# dataset_test.change_to_Doc2Vec(doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = AssemblePredicted(data_name, dataset_train, dataset_validate, dataset_test, iteration=2000, batch_size=4098, hidden_size=[300,1500,1500,1500,1500,1500,1500,1500,1500,1500,1500,1500], target_hidden_size=[60,60,60,60,60,60,60,60,60,60,60], use_dropout=True, stopping_time=200, start_level=0, end_level=1000)\n",
    "# model = AssembleNoLabel(data_name, dataset_train, dataset_validate, dataset_test, iteration=2000, batch_size=5000, hidden_size=[300,1500,1500,1500,1500,1500,1500,1500,1500,1500,1500,1500], use_dropout=False, start_level=10000, end_level=10000, stopping_time=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 1.000 Epoch: 66/2000 Batch: 100/100 Loss: 1.347 4 Training F1 macro: 0.833 Validate F1 macro: 0.554\n",
      "Level: 1.000 Epoch: 132/2000 Batch: 100/100 Loss: 0.255 Training F1 macro: 0.974 Validate F1 macro: 0.812\n",
      "Level: 1.000 Epoch: 198/2000 Batch: 100/100 Loss: 0.157 Training F1 macro: 0.979 Validate F1 macro: 0.790\n",
      "Level: 1.000 Epoch: 264/2000 Batch: 100/100 Loss: 0.139 Training F1 macro: 0.980 Validate F1 macro: 0.814\n",
      "Level: 1.000 Epoch: 330/2000 Batch: 100/100 Loss: 0.127 Training F1 macro: 0.982 Validate F1 macro: 0.833\n",
      "Level: 1.000 Epoch: 396/2000 Batch: 100/100 Loss: 0.122 Training F1 macro: 0.983 Validate F1 macro: 0.851\n",
      "Level: 1.000 Epoch: 416/2000 Batch: 100/100 Loss: 0.118 Stopping F1 macro: 0.983 Validate F1 macro: 0.870\n",
      "\n",
      "Level: 2.000 Epoch: 66/2000 Batch: 100/100 Loss: 1.429 6 Training F1 macro: 0.722 Validate F1 macro: 0.511\n",
      "Level: 2.000 Epoch: 132/2000 Batch: 100/100 Loss: 1.068 Training F1 macro: 0.802 Validate F1 macro: 0.615\n",
      "Level: 2.000 Epoch: 198/2000 Batch: 100/100 Loss: 0.870 Training F1 macro: 0.844 Validate F1 macro: 0.632\n",
      "Level: 2.000 Epoch: 264/2000 Batch: 100/100 Loss: 0.789 Training F1 macro: 0.863 Validate F1 macro: 0.655\n",
      "Level: 2.000 Epoch: 330/2000 Batch: 100/100 Loss: 0.768 Training F1 macro: 0.866 Validate F1 macro: 0.662\n",
      "Level: 2.000 Epoch: 396/2000 Batch: 100/100 Loss: 0.730 Training F1 macro: 0.875 Validate F1 macro: 0.655\n",
      "Level: 2.000 Epoch: 462/2000 Batch: 100/100 Loss: 0.710 Training F1 macro: 0.877 Validate F1 macro: 0.670\n",
      "Level: 2.000 Epoch: 528/2000 Batch: 100/100 Loss: 0.698 Training F1 macro: 0.881 Validate F1 macro: 0.669\n",
      "Level: 2.000 Epoch: 594/2000 Batch: 100/100 Loss: 0.709 Training F1 macro: 0.879 Validate F1 macro: 0.657\n",
      "Level: 2.000 Epoch: 660/2000 Batch: 100/100 Loss: 0.700 Training F1 macro: 0.884 Validate F1 macro: 0.674\n",
      "Level: 2.000 Epoch: 726/2000 Batch: 100/100 Loss: 0.683 Training F1 macro: 0.884 Validate F1 macro: 0.676\n",
      "Level: 2.000 Epoch: 792/2000 Batch: 100/100 Loss: 0.687 Training F1 macro: 0.885 Validate F1 macro: 0.665\n",
      "Level: 2.000 Epoch: 858/2000 Batch: 100/100 Loss: 0.691 Training F1 macro: 0.885 Validate F1 macro: 0.663\n",
      "Level: 2.000 Epoch: 924/2000 Batch: 100/100 Loss: 0.656 Training F1 macro: 0.889 Validate F1 macro: 0.664\n",
      "Level: 2.000 Epoch: 940/2000 Batch: 100/100 Loss: 0.675 Stopping F1 macro: 0.888 Validate F1 macro: 0.690\n",
      "\n",
      "Level: 3.000 Epoch: 66/2000 Batch: 100/100 Loss: 2.718  Training F1 macro: 0.537 Validate F1 macro: 0.366\n",
      "Level: 3.000 Epoch: 132/2000 Batch: 100/100 Loss: 1.664 Training F1 macro: 0.768 Validate F1 macro: 0.555\n",
      "Level: 3.000 Epoch: 198/2000 Batch: 100/100 Loss: 1.217 Training F1 macro: 0.837 Validate F1 macro: 0.619\n",
      "Level: 3.000 Epoch: 264/2000 Batch: 100/100 Loss: 1.079 Training F1 macro: 0.857 Validate F1 macro: 0.644\n",
      "Level: 3.000 Epoch: 330/2000 Batch: 100/100 Loss: 1.021 Training F1 macro: 0.868 Validate F1 macro: 0.654\n",
      "Level: 3.000 Epoch: 396/2000 Batch: 100/100 Loss: 0.983 Training F1 macro: 0.874 Validate F1 macro: 0.661\n",
      "Level: 3.000 Epoch: 462/2000 Batch: 100/100 Loss: 0.961 Training F1 macro: 0.877 Validate F1 macro: 0.667\n",
      "Level: 3.000 Epoch: 528/2000 Batch: 100/100 Loss: 0.943 Training F1 macro: 0.880 Validate F1 macro: 0.670\n",
      "Level: 3.000 Epoch: 594/2000 Batch: 100/100 Loss: 0.934 Training F1 macro: 0.883 Validate F1 macro: 0.673\n",
      "Level: 3.000 Epoch: 660/2000 Batch: 100/100 Loss: 0.925 Training F1 macro: 0.881 Validate F1 macro: 0.669\n",
      "Level: 3.000 Epoch: 726/2000 Batch: 100/100 Loss: 0.909 Training F1 macro: 0.884 Validate F1 macro: 0.670\n",
      "Level: 3.000 Epoch: 792/2000 Batch: 100/100 Loss: 0.900 Training F1 macro: 0.885 Validate F1 macro: 0.671\n",
      "Level: 3.000 Epoch: 806/2000 Batch: 100/100 Loss: 0.906 Stopping F1 macro: 0.884 Validate F1 macro: 0.675\n",
      "\n",
      "Level: 4.000 Epoch: 66/2000 Batch: 100/100 Loss: 3.551 9 Training F1 macro: 0.386 Validate F1 macro: 0.230\n",
      "Level: 4.000 Epoch: 132/2000 Batch: 100/100 Loss: 1.725 Training F1 macro: 0.800 Validate F1 macro: 0.559\n",
      "Level: 4.000 Epoch: 198/2000 Batch: 100/100 Loss: 1.124 Training F1 macro: 0.882 Validate F1 macro: 0.645\n",
      "Level: 4.000 Epoch: 264/2000 Batch: 100/100 Loss: 0.947 Training F1 macro: 0.906 Validate F1 macro: 0.675\n",
      "Level: 4.000 Epoch: 330/2000 Batch: 100/100 Loss: 0.867 Training F1 macro: 0.916 Validate F1 macro: 0.685\n",
      "Level: 4.000 Epoch: 396/2000 Batch: 100/100 Loss: 0.826 Training F1 macro: 0.921 Validate F1 macro: 0.692\n",
      "Level: 4.000 Epoch: 462/2000 Batch: 100/100 Loss: 0.796 Training F1 macro: 0.925 Validate F1 macro: 0.698\n",
      "Level: 4.000 Epoch: 528/2000 Batch: 100/100 Loss: 0.775 Training F1 macro: 0.927 Validate F1 macro: 0.697\n",
      "Level: 4.000 Epoch: 594/2000 Batch: 100/100 Loss: 0.757 Training F1 macro: 0.928 Validate F1 macro: 0.697\n",
      "Level: 4.000 Epoch: 660/2000 Batch: 100/100 Loss: 0.747 Training F1 macro: 0.929 Validate F1 macro: 0.696\n",
      "Level: 4.000 Epoch: 726/2000 Batch: 100/100 Loss: 0.735 Training F1 macro: 0.931 Validate F1 macro: 0.697\n",
      "Level: 4.000 Epoch: 792/2000 Batch: 100/100 Loss: 0.731 Training F1 macro: 0.932 Validate F1 macro: 0.700\n",
      "Level: 4.000 Epoch: 858/2000 Batch: 100/100 Loss: 0.724 Training F1 macro: 0.932 Validate F1 macro: 0.700\n",
      "Level: 4.000 Epoch: 924/2000 Batch: 100/100 Loss: 0.718 Training F1 macro: 0.932 Validate F1 macro: 0.700\n",
      "Level: 4.000 Epoch: 990/2000 Batch: 100/100 Loss: 0.713 Training F1 macro: 0.933 Validate F1 macro: 0.701\n",
      "Level: 4.000 Epoch: 1056/2000 Batch: 100/100 Loss: 0.704 Training F1 macro: 0.935 Validate F1 macro: 0.704\n",
      "Level: 4.000 Epoch: 1122/2000 Batch: 100/100 Loss: 0.705 Training F1 macro: 0.935 Validate F1 macro: 0.702\n",
      "Level: 4.000 Epoch: 1188/2000 Batch: 100/100 Loss: 0.698 Training F1 macro: 0.935 Validate F1 macro: 0.701\n",
      "Level: 4.000 Epoch: 1254/2000 Batch: 100/100 Loss: 0.702 Training F1 macro: 0.935 Validate F1 macro: 0.703\n",
      "Level: 4.000 Epoch: 1320/2000 Batch: 100/100 Loss: 0.695 Training F1 macro: 0.935 Validate F1 macro: 0.701\n",
      "Level: 4.000 Epoch: 1335/2000 Batch: 100/100 Loss: 0.695 Stopping F1 macro: 0.936 Validate F1 macro: 0.709\n",
      "\n",
      "Level: 5.000 Epoch: 66/2000 Batch: 100/100 Loss: 3.593   Training F1 macro: 0.374 Validate F1 macro: 0.195\n",
      "Level: 5.000 Epoch: 132/2000 Batch: 100/100 Loss: 1.339 Training F1 macro: 0.869 Validate F1 macro: 0.596\n",
      "Level: 5.000 Epoch: 198/2000 Batch: 100/100 Loss: 0.751 Training F1 macro: 0.932 Validate F1 macro: 0.682\n",
      "Level: 5.000 Epoch: 264/2000 Batch: 100/100 Loss: 0.588 Training F1 macro: 0.953 Validate F1 macro: 0.720\n",
      "Level: 5.000 Epoch: 330/2000 Batch: 100/100 Loss: 0.517 Training F1 macro: 0.961 Validate F1 macro: 0.733\n",
      "Level: 5.000 Epoch: 396/2000 Batch: 100/100 Loss: 0.476 Training F1 macro: 0.965 Validate F1 macro: 0.743\n",
      "Level: 5.000 Epoch: 462/2000 Batch: 100/100 Loss: 0.454 Training F1 macro: 0.968 Validate F1 macro: 0.747\n",
      "Level: 5.000 Epoch: 528/2000 Batch: 100/100 Loss: 0.438 Training F1 macro: 0.969 Validate F1 macro: 0.747\n",
      "Level: 5.000 Epoch: 594/2000 Batch: 100/100 Loss: 0.427 Training F1 macro: 0.969 Validate F1 macro: 0.747\n",
      "Level: 5.000 Epoch: 660/2000 Batch: 100/100 Loss: 0.417 Training F1 macro: 0.970 Validate F1 macro: 0.746\n",
      "Level: 5.000 Epoch: 726/2000 Batch: 100/100 Loss: 0.408 Training F1 macro: 0.970 Validate F1 macro: 0.743\n",
      "Level: 5.000 Epoch: 783/2000 Batch: 91/100 Loss: 0.414  "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.tuning_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# f1_macro, f1_micro, f1_each = model.evaluate(\"train\", correction=False)\n",
    "# print(\"F1 macro: %.4f F1 micro: %.4f\" % (f1_macro, f1_micro))\n",
    "# for level, (macro, micro) in enumerate(f1_each):\n",
    "#     print(\"Level: %d F1 macro: %.4f F1 micro: %.4f\" % (level, macro, micro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# f1_macro, f1_micro, f1_each = model.evaluate(\"validate\", correction=False)\n",
    "# print(\"F1 macro: %.4f F1 micro: %.4f\" % (f1_macro, f1_micro))\n",
    "# for level, (macro, micro) in enumerate(f1_each):\n",
    "#     print(\"Level: %d F1 macro: %.4f F1 micro: %.4f\" % (level, macro, micro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.export_result(\"test\", mandatory_leaf=True, correction=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
