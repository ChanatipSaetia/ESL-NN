{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import preparation\n",
    "import data.hierarchy as hie\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_name = \"wipo_d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/%s/hierarchy.txt' % data_name, 'w') as f1:\n",
    "    with open('data/%s/%s.ht' % (data_name,data_name)) as f:\n",
    "        for l in f:\n",
    "            split = l.strip().split(':')\n",
    "            p = split[0]\n",
    "            c = split[1].split(',')\n",
    "            for cc in c:\n",
    "                f1.write('%s %s\\n'% (p,cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hierarchy, parent_of, all_name, name_to_index, level = hie.reindex_hierarchy(\n",
    "                '%s/hierarchy.txt' % data_name)\n",
    "hie.save_hierarchy(\"%s/hierarchy.pickle\" % data_name, hierarchy,\n",
    "                   parent_of, all_name, name_to_index, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for mode in ['train', 'test']:\n",
    "    for i in range(1,6):\n",
    "        datas, labels = preparation.import_data('%s/folds/%s_fold%d.dat.%s' % (data_name, data_name, i, mode))\n",
    "        new_labels = preparation.map_index_of_label('%s/hierarchy.pickle' % data_name, labels)\n",
    "        if mode == 'train':\n",
    "            train_data, validate_data, train_target, validate_target = train_test_split(\n",
    "                datas, new_labels, test_size=0.1, random_state=12345)\n",
    "            preparation.save_data_in_pickle('%s/fold/data_%d.pickle.%s' % (data_name, i, mode), train_data, train_target)\n",
    "            preparation.save_data_in_pickle('%s/fold/data_%d.pickle.%s' % (data_name, i, \"validate\"), validate_data, validate_target)\n",
    "        else:\n",
    "            preparation.save_data_in_pickle('%s/fold/data_%d.pickle.%s' % (data_name, i, mode), datas, new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "20\n",
      "160\n",
      "645\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(level)-1):\n",
    "    print(level[i+1] - level[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import Dataset\n",
    "from embedding import Doc2Vec\n",
    "from assemble_classifier import AssembleNoLabel, AssemblePredicted,AssemblePredictedHidden ,AssemblePredictedHiddenAdd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "train doc2vec\n",
      "Doc2Vec by Gensim\n",
      "Epoch: 20 Similar: 2.49\n",
      "Epoch: 40 Similar: 13.70\n",
      "Epoch: 60 Similar: 17.03\n",
      "Epoch: 80 Similar: 17.39\n",
      "Epoch: 100 Similar: 16.90\n",
      "Epoch: 120 Similar: 17.30\n",
      "Epoch: 140 Similar: 16.74\n",
      "Epoch: 160 Similar: 17.36\n",
      "Epoch: 180 Similar: 17.95\n",
      "Epoch: 200 Similar: 17.53\n",
      "Epoch: 220 Similar: 17.23\n",
      "Epoch: 240 Similar: 17.70\n",
      "Stopping Similar: 17.70\n",
      "train main classifier\n",
      "Level: 1.000 Epoch: 666/20000 Batch: 1/1 Loss: 0.185 Training F1 macro: 0.984 Validate F1 macro: 0.674\n",
      "Level: 1.000 Epoch: 1166/20000 Batch: 1/1 Loss: 0.101 Stopping F1 macro: 0.994 Validate F1 macro: 0.706\n",
      "\n",
      "Level: 2.000 Epoch: 666/20000 Batch: 1/1 Loss: 0.124 Training F1 macro: 0.997 Validate F1 macro: 0.517\n",
      "Level: 2.000 Epoch: 1166/20000 Batch: 1/1 Loss: 0.055 Stopping F1 macro: 1.000 Validate F1 macro: 0.554\n",
      "\n",
      "Level: 3.000 Epoch: 666/20000 Batch: 1/1 Loss: 1.052 Training F1 macro: 0.841 Validate F1 macro: 0.052\n",
      "Level: 3.000 Epoch: 1332/20000 Batch: 1/1 Loss: 0.377 Training F1 macro: 0.940 Validate F1 macro: 0.091\n",
      "Level: 3.000 Epoch: 1998/20000 Batch: 1/1 Loss: 0.213 Training F1 macro: 0.948 Validate F1 macro: 0.101\n",
      "Level: 3.000 Epoch: 2664/20000 Batch: 1/1 Loss: 0.151 Training F1 macro: 0.949 Validate F1 macro: 0.101\n",
      "Level: 3.000 Epoch: 3023/20000 Batch: 1/1 Loss: 0.116 Stopping F1 macro: 0.950 Validate F1 macro: 0.110\n",
      "\n",
      "Level: 4.000 Epoch: 666/20000 Batch: 1/1 Loss: 1.591  Training F1 macro: 0.509 Validate F1 macro: 0.003\n",
      "Level: 4.000 Epoch: 1332/20000 Batch: 1/1 Loss: 0.546 Training F1 macro: 0.838 Validate F1 macro: 0.012\n",
      "Level: 4.000 Epoch: 1998/20000 Batch: 1/1 Loss: 0.278 Training F1 macro: 0.858 Validate F1 macro: 0.017\n",
      "Level: 4.000 Epoch: 2294/20000 Batch: 1/1 Loss: 0.260 Stopping F1 macro: 0.859 Validate F1 macro: 0.019\n",
      "\n",
      "evaluate\n",
      "F1 macro: 0.8393 F1 micro: 0.9024\n",
      "Level: 0 F1 macro: 0.9545 F1 micro: 0.9617\n",
      "Level: 1 F1 macro: 0.9457 F1 micro: 0.9447\n",
      "Level: 2 F1 macro: 0.8927 F1 micro: 0.8507\n",
      "Level: 3 F1 macro: 0.8215 F1 micro: 0.8630\n",
      "F1 macro: 0.0694 F1 micro: 0.4971\n",
      "Level: 0 F1 macro: 0.6385 F1 micro: 0.7697\n",
      "Level: 1 F1 macro: 0.5460 F1 micro: 0.5981\n",
      "Level: 2 F1 macro: 0.1350 F1 micro: 0.3476\n",
      "Level: 3 F1 macro: 0.0322 F1 micro: 0.2470\n",
      "F1 macro: 0.0981 F1 micro: 0.4984\n",
      "Level: 0 F1 macro: 0.6537 F1 micro: 0.7688\n",
      "Level: 1 F1 macro: 0.5456 F1 micro: 0.6126\n",
      "Level: 2 F1 macro: 0.1735 F1 micro: 0.3927\n",
      "Level: 3 F1 macro: 0.0595 F1 micro: 0.2051\n",
      "\n",
      "\n",
      "train doc2vec\n",
      "Doc2Vec by Gensim\n",
      "Epoch: 20 Similar: 3.64\n",
      "Epoch: 40 Similar: 14.72\n",
      "Epoch: 60 Similar: 17.49\n",
      "Epoch: 80 Similar: 17.20\n",
      "Epoch: 100 Similar: 16.72\n",
      "Epoch: 120 Similar: 17.24\n",
      "Epoch: 140 Similar: 17.33\n",
      "Epoch: 160 Similar: 17.65\n",
      "Epoch: 180 Similar: 17.17\n",
      "Epoch: 200 Similar: 17.15\n",
      "Epoch: 220 Similar: 17.27\n",
      "Epoch: 240 Similar: 17.13\n",
      "Stopping Similar: 17.13\n",
      "train main classifier\n",
      "Level: 1.000 Epoch: 666/20000 Batch: 1/1 Loss: 0.208 Training F1 macro: 0.977 Validate F1 macro: 0.635\n",
      "Level: 1.000 Epoch: 1166/20000 Batch: 1/1 Loss: 0.118 Stopping F1 macro: 0.995 Validate F1 macro: 0.659\n",
      "\n",
      "Level: 2.000 Epoch: 666/20000 Batch: 1/1 Loss: 0.147 Training F1 macro: 0.998 Validate F1 macro: 0.501\n",
      "Level: 2.000 Epoch: 1332/20000 Batch: 1/1 Loss: 0.051 Training F1 macro: 1.000 Validate F1 macro: 0.511\n",
      "Level: 2.000 Epoch: 1761/20000 Batch: 1/1 Loss: 0.040 Stopping F1 macro: 1.000 Validate F1 macro: 0.537\n",
      "\n",
      "Level: 3.000 Epoch: 666/20000 Batch: 1/1 Loss: 1.046 Training F1 macro: 0.854 Validate F1 macro: 0.070\n",
      "Level: 3.000 Epoch: 1332/20000 Batch: 1/1 Loss: 0.410 Training F1 macro: 0.930 Validate F1 macro: 0.086\n",
      "Level: 3.000 Epoch: 1998/20000 Batch: 1/1 Loss: 0.240 Training F1 macro: 0.940 Validate F1 macro: 0.102\n",
      "Level: 3.000 Epoch: 2640/20000 Batch: 1/1 Loss: 0.170 Stopping F1 macro: 0.942 Validate F1 macro: 0.108\n",
      "\n",
      "Level: 4.000 Epoch: 666/20000 Batch: 1/1 Loss: 1.688  Training F1 macro: 0.448 Validate F1 macro: 0.006\n",
      "Level: 4.000 Epoch: 1332/20000 Batch: 1/1 Loss: 0.602 Training F1 macro: 0.830 Validate F1 macro: 0.011\n",
      "Level: 4.000 Epoch: 1998/20000 Batch: 1/1 Loss: 0.334 Training F1 macro: 0.857 Validate F1 macro: 0.010\n",
      "Level: 4.000 Epoch: 2186/20000 Batch: 1/1 Loss: 0.274 Stopping F1 macro: 0.858 Validate F1 macro: 0.013\n",
      "\n",
      "evaluate\n",
      "F1 macro: 0.8279 F1 micro: 0.8992\n",
      "Level: 0 F1 macro: 0.9647 F1 micro: 0.9758\n",
      "Level: 1 F1 macro: 0.9612 F1 micro: 0.9610\n",
      "Level: 2 F1 macro: 0.8827 F1 micro: 0.8563\n",
      "Level: 3 F1 macro: 0.8087 F1 micro: 0.8227\n",
      "F1 macro: 0.0595 F1 micro: 0.4489\n",
      "Level: 0 F1 macro: 0.6189 F1 micro: 0.7314\n",
      "Level: 1 F1 macro: 0.5377 F1 micro: 0.5509\n",
      "Level: 2 F1 macro: 0.1236 F1 micro: 0.3142\n",
      "Level: 3 F1 macro: 0.0227 F1 micro: 0.1727\n",
      "F1 macro: 0.0790 F1 micro: 0.4794\n",
      "Level: 0 F1 macro: 0.6411 F1 micro: 0.7530\n",
      "Level: 1 F1 macro: 0.5281 F1 micro: 0.6083\n",
      "Level: 2 F1 macro: 0.1448 F1 micro: 0.3528\n",
      "Level: 3 F1 macro: 0.0427 F1 micro: 0.1755\n",
      "\n",
      "\n",
      "train doc2vec\n",
      "Doc2Vec by Gensim\n",
      "Epoch: 20 Similar: 4.16\n",
      "Epoch: 40 Similar: 14.96\n",
      "Epoch: 60 Similar: 17.58\n",
      "Epoch: 80 Similar: 16.31\n",
      "Epoch: 100 Similar: 15.12\n",
      "Epoch: 120 Similar: 15.56\n",
      "Epoch: 140 Similar: 15.53\n",
      "Epoch: 160 Similar: 15.63\n",
      "Epoch: 180 Similar: 15.25\n",
      "Epoch: 200 Similar: 15.77\n",
      "Epoch: 220 Similar: 15.20\n",
      "Epoch: 240 Similar: 15.46\n",
      "Stopping Similar: 15.46\n",
      "train main classifier\n",
      "Level: 1.000 Epoch: 666/20000 Batch: 1/1 Loss: 0.695 Training F1 macro: 0.866 Validate F1 macro: 0.612\n",
      "Level: 1.000 Epoch: 1332/20000 Batch: 1/1 Loss: 0.501 Training F1 macro: 0.913 Validate F1 macro: 0.648\n",
      "Level: 1.000 Epoch: 1913/20000 Batch: 1/1 Loss: 0.391 Stopping F1 macro: 0.934 Validate F1 macro: 0.659\n",
      "\n",
      "Level: 2.000 Epoch: 666/20000 Batch: 1/1 Loss: 0.658 Training F1 macro: 0.925 Validate F1 macro: 0.481\n",
      "Level: 2.000 Epoch: 1166/20000 Batch: 1/1 Loss: 0.380 Stopping F1 macro: 0.975 Validate F1 macro: 0.539\n",
      "\n",
      "Level: 3.000 Epoch: 666/20000 Batch: 1/1 Loss: 2.205 Training F1 macro: 0.370 Validate F1 macro: 0.043\n",
      "Level: 3.000 Epoch: 1332/20000 Batch: 1/1 Loss: 1.291 Training F1 macro: 0.777 Validate F1 macro: 0.070\n",
      "Level: 3.000 Epoch: 1998/20000 Batch: 1/1 Loss: 0.876 Training F1 macro: 0.855 Validate F1 macro: 0.078\n",
      "Level: 3.000 Epoch: 2664/20000 Batch: 1/1 Loss: 0.631 Training F1 macro: 0.888 Validate F1 macro: 0.082\n",
      "Level: 3.000 Epoch: 3278/20000 Batch: 1/1 Loss: 0.499 Stopping F1 macro: 0.902 Validate F1 macro: 0.095\n",
      "\n",
      "Level: 4.000 Epoch: 666/20000 Batch: 1/1 Loss: 3.374  Training F1 macro: 0.060 Validate F1 macro: 0.002\n",
      "Level: 4.000 Epoch: 1332/20000 Batch: 1/1 Loss: 1.857 Training F1 macro: 0.373 Validate F1 macro: 0.004\n",
      "Level: 4.000 Epoch: 1606/20000 Batch: 1/1 Loss: 1.556 Stopping F1 macro: 0.519 Validate F1 macro: 0.005\n",
      "\n",
      "evaluate\n",
      "F1 macro: 0.5743 F1 micro: 0.6235\n",
      "Level: 0 F1 macro: 0.8300 F1 micro: 0.8943\n",
      "Level: 1 F1 macro: 0.7301 F1 micro: 0.7670\n",
      "Level: 2 F1 macro: 0.5957 F1 micro: 0.5396\n",
      "Level: 3 F1 macro: 0.5614 F1 micro: 0.4577\n",
      "F1 macro: 0.0499 F1 micro: 0.3656\n",
      "Level: 0 F1 macro: 0.6349 F1 micro: 0.7289\n",
      "Level: 1 F1 macro: 0.4673 F1 micro: 0.5168\n",
      "Level: 2 F1 macro: 0.0961 F1 micro: 0.2447\n",
      "Level: 3 F1 macro: 0.0191 F1 micro: 0.1106\n",
      "F1 macro: 0.0601 F1 micro: 0.3699\n",
      "Level: 0 F1 macro: 0.6093 F1 micro: 0.7625\n",
      "Level: 1 F1 macro: 0.4714 F1 micro: 0.5325\n",
      "Level: 2 F1 macro: 0.1090 F1 micro: 0.2575\n",
      "Level: 3 F1 macro: 0.0292 F1 micro: 0.1104\n",
      "\n",
      "\n",
      "train doc2vec\n",
      "Doc2Vec by Gensim\n",
      "Epoch: 20 Similar: 2.97\n",
      "Epoch: 40 Similar: 14.15\n",
      "Epoch: 60 Similar: 16.58\n",
      "Epoch: 80 Similar: 16.20\n",
      "Epoch: 100 Similar: 14.93\n",
      "Epoch: 120 Similar: 15.02\n",
      "Epoch: 140 Similar: 14.99\n",
      "Epoch: 160 Similar: 14.47\n",
      "Epoch: 180 Similar: 14.68\n",
      "Epoch: 200 Similar: 14.68\n",
      "Epoch: 220 Similar: 14.76\n",
      "Epoch: 240 Similar: 14.42\n",
      "Stopping Similar: 14.42\n",
      "train main classifier\n",
      "Level: 1.000 Epoch: 666/20000 Batch: 1/1 Loss: 0.667 Training F1 macro: 0.880 Validate F1 macro: 0.594\n",
      "Level: 1.000 Epoch: 1292/20000 Batch: 1/1 Loss: 0.438 Stopping F1 macro: 0.934 Validate F1 macro: 0.614\n",
      "\n",
      "Level: 2.000 Epoch: 666/20000 Batch: 1/1 Loss: 0.705 Training F1 macro: 0.924 Validate F1 macro: 0.518\n",
      "Level: 2.000 Epoch: 1166/20000 Batch: 1/1 Loss: 0.429 Stopping F1 macro: 0.978 Validate F1 macro: 0.563\n",
      "\n",
      "Level: 3.000 Epoch: 666/20000 Batch: 1/1 Loss: 2.078 Training F1 macro: 0.451 Validate F1 macro: 0.047\n",
      "Level: 3.000 Epoch: 1332/20000 Batch: 1/1 Loss: 1.087 Training F1 macro: 0.834 Validate F1 macro: 0.063\n",
      "Level: 3.000 Epoch: 1998/20000 Batch: 1/1 Loss: 0.691 Training F1 macro: 0.897 Validate F1 macro: 0.059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 3.000 Epoch: 2664/20000 Batch: 1/1 Loss: 0.510 Training F1 macro: 0.918 Validate F1 macro: 0.072\n",
      "Level: 3.000 Epoch: 3255/20000 Batch: 1/1 Loss: 0.432 Stopping F1 macro: 0.928 Validate F1 macro: 0.081\n",
      "\n",
      "Level: 4.000 Epoch: 666/20000 Batch: 1/1 Loss: 3.148  Training F1 macro: 0.063 Validate F1 macro: 0.001\n",
      "Level: 4.000 Epoch: 1332/20000 Batch: 1/1 Loss: 1.720 Training F1 macro: 0.429 Validate F1 macro: 0.004\n",
      "Level: 4.000 Epoch: 1998/20000 Batch: 1/1 Loss: 1.161 Training F1 macro: 0.693 Validate F1 macro: 0.009\n",
      "Level: 4.000 Epoch: 2664/20000 Batch: 1/1 Loss: 0.806 Training F1 macro: 0.782 Validate F1 macro: 0.009\n",
      "Level: 4.000 Epoch: 2665/20000 Batch: 1/1 Loss: 0.831 Stopping F1 macro: 0.781 Validate F1 macro: 0.011\n",
      "\n",
      "evaluate\n",
      "F1 macro: 0.6562 F1 micro: 0.6824\n",
      "Level: 0 F1 macro: 0.8903 F1 micro: 0.9245\n",
      "Level: 1 F1 macro: 0.7689 F1 micro: 0.8007\n",
      "Level: 2 F1 macro: 0.6959 F1 micro: 0.5833\n",
      "Level: 3 F1 macro: 0.6404 F1 micro: 0.5499\n",
      "F1 macro: 0.0511 F1 micro: 0.3595\n",
      "Level: 0 F1 macro: 0.6586 F1 micro: 0.7172\n",
      "Level: 1 F1 macro: 0.4815 F1 micro: 0.5051\n",
      "Level: 2 F1 macro: 0.0946 F1 micro: 0.2505\n",
      "Level: 3 F1 macro: 0.0204 F1 micro: 0.0938\n",
      "F1 macro: 0.0606 F1 micro: 0.3584\n",
      "Level: 0 F1 macro: 0.5612 F1 micro: 0.7024\n",
      "Level: 1 F1 macro: 0.4536 F1 micro: 0.5134\n",
      "Level: 2 F1 macro: 0.1056 F1 micro: 0.2446\n",
      "Level: 3 F1 macro: 0.0319 F1 micro: 0.1176\n",
      "\n",
      "\n",
      "train doc2vec\n",
      "Doc2Vec by Gensim\n",
      "Epoch: 20 Similar: 3.81\n",
      "Epoch: 40 Similar: 14.40\n",
      "Epoch: 60 Similar: 16.26\n",
      "Epoch: 80 Similar: 15.99\n",
      "Epoch: 100 Similar: 14.96\n",
      "Epoch: 120 Similar: 14.68\n",
      "Epoch: 140 Similar: 14.54\n",
      "Epoch: 160 Similar: 14.92\n",
      "Epoch: 180 Similar: 14.48\n",
      "Epoch: 200 Similar: 14.33\n",
      "Epoch: 220 Similar: 14.10\n",
      "Epoch: 240 Similar: 14.11\n",
      "Stopping Similar: 14.11\n",
      "train main classifier\n",
      "Level: 1.000 Epoch: 666/20000 Batch: 1/1 Loss: 0.704 Training F1 macro: 0.883 Validate F1 macro: 0.577\n",
      "Level: 1.000 Epoch: 1332/20000 Batch: 1/1 Loss: 0.502 Training F1 macro: 0.925 Validate F1 macro: 0.633\n",
      "Level: 1.000 Epoch: 1998/20000 Batch: 1/1 Loss: 0.356 Training F1 macro: 0.946 Validate F1 macro: 0.637\n",
      "Level: 1.000 Epoch: 2304/20000 Batch: 1/1 Loss: 0.346 Stopping F1 macro: 0.955 Validate F1 macro: 0.662\n",
      "\n",
      "Level: 2.000 Epoch: 666/20000 Batch: 1/1 Loss: 0.605 Training F1 macro: 0.939 Validate F1 macro: 0.488\n",
      "Level: 2.000 Epoch: 1166/20000 Batch: 1/1 Loss: 0.346 Stopping F1 macro: 0.982 Validate F1 macro: 0.514\n",
      "\n",
      "Level: 3.000 Epoch: 666/20000 Batch: 1/1 Loss: 2.005 Training F1 macro: 0.480 Validate F1 macro: 0.049\n",
      "Level: 3.000 Epoch: 1332/20000 Batch: 1/1 Loss: 1.051 Training F1 macro: 0.834 Validate F1 macro: 0.068\n",
      "Level: 3.000 Epoch: 1998/20000 Batch: 1/1 Loss: 0.664 Training F1 macro: 0.903 Validate F1 macro: 0.071\n",
      "Level: 3.000 Epoch: 2480/20000 Batch: 1/1 Loss: 0.520 Stopping F1 macro: 0.914 Validate F1 macro: 0.083\n",
      "\n",
      "Level: 4.000 Epoch: 666/20000 Batch: 1/1 Loss: 3.221  Training F1 macro: 0.061 Validate F1 macro: 0.000\n",
      "Level: 4.000 Epoch: 1332/20000 Batch: 1/1 Loss: 1.728 Training F1 macro: 0.417 Validate F1 macro: 0.000\n",
      "Level: 4.000 Epoch: 1637/20000 Batch: 1/1 Loss: 1.414 Stopping F1 macro: 0.583 Validate F1 macro: 0.003\n",
      "\n",
      "evaluate\n",
      "F1 macro: 0.5810 F1 micro: 0.6093\n",
      "Level: 0 F1 macro: 0.8243 F1 micro: 0.8862\n",
      "Level: 1 F1 macro: 0.7147 F1 micro: 0.7465\n",
      "Level: 2 F1 macro: 0.5910 F1 micro: 0.5095\n",
      "Level: 3 F1 macro: 0.5717 F1 micro: 0.4623\n",
      "F1 macro: 0.0529 F1 micro: 0.3704\n",
      "Level: 0 F1 macro: 0.6312 F1 micro: 0.7485\n",
      "Level: 1 F1 macro: 0.4687 F1 micro: 0.5281\n",
      "Level: 2 F1 macro: 0.1004 F1 micro: 0.2587\n",
      "Level: 3 F1 macro: 0.0220 F1 micro: 0.1165\n",
      "F1 macro: 0.0669 F1 micro: 0.3700\n",
      "Level: 0 F1 macro: 0.6212 F1 micro: 0.7398\n",
      "Level: 1 F1 macro: 0.4611 F1 micro: 0.5382\n",
      "Level: 2 F1 macro: 0.1219 F1 micro: 0.2676\n",
      "Level: 3 F1 macro: 0.0350 F1 micro: 0.1187\n",
      "CPU times: user 17min 38s, sys: 2min 17s, total: 19min 56s\n",
      "Wall time: 13min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for fold in range(1, 6):\n",
    "    print()\n",
    "    print()\n",
    "    if os.path.isdir('data/%s/output' % data_name):\n",
    "        shutil.rmtree('data/%s/output' % data_name)\n",
    "\n",
    "    dataset_train = Dataset(data_name, fold, \"train\")\n",
    "    dataset_validate = Dataset(data_name, fold, \"validate\")\n",
    "    dataset_test = Dataset(data_name, fold, \"test\")\n",
    "\n",
    "    print(\"train doc2vec\")http://localhost:8888/notebooks/Demo-5fold.ipynb#\n",
    "    doc2vec = Doc2Vec(dataset_train.number_of_classes(), size=100, epoch=1000)\n",
    "    doc2vec.fit(dataset_train.datas, dataset_train.labels, dataset_validate.datas, dataset_validate.labels)\n",
    "\n",
    "    dataset_train.change_to_Doc2Vec(doc2vec)\n",
    "    dataset_validate.change_to_Doc2Vec(doc2vec)\n",
    "    dataset_test.change_to_Doc2Vec(doc2vec)\n",
    "    \n",
    "    print(\"train main classifier\")\n",
    "    model = AssemblePredicted(data_name, dataset_train, dataset_validate, dataset_test, iteration=20000, batch_size=20000, hidden_size=[300,1500,1500,1500], target_hidden_size=[60,60,60], use_dropout=True, start_level=0)\n",
    "#     model = AssemblePredictedHidden(data_name, dataset_train, dataset_validate, dataset_test, iteration=20000, batch_size=2000, hidden_size=[300,1500,2000,2500], use_dropout=False, start_level=0, end_level=100)\n",
    "    #     model = AssemblePredicted(data_name, dataset_train, dataset_validate, dataset_test, iteration=20000, batch_size=20000, hidden_size=[300,1500,2000,2500,2500,1500,1500,1500,300], target_hidden_size=[60,100,150,200,200,150,150,100,60], use_dropout=True, start_level=0)\n",
    "    model.train()\n",
    "    model.tuning_threshold()\n",
    "\n",
    "    print(\"evaluate\")\n",
    "    f1_macro, f1_micro, f1_each = model.evaluate(\"train\", correction=True)\n",
    "    print(\"F1 macro: %.4f F1 micro: %.4f\" % (f1_macro, f1_micro))\n",
    "    for level, (macro, micro) in enumerate(f1_each):\n",
    "        print(\"Level: %d F1 macro: %.4f F1 micro: %.4f\" % (level, macro, micro))\n",
    "    f1_macro, f1_micro, f1_each = model.evaluate(\"validate\", correction=True)\n",
    "    print(\"F1 macro: %.4f F1 micro: %.4f\" % (f1_macro, f1_micro))\n",
    "    for level, (macro, micro) in enumerate(f1_each):\n",
    "        print(\"Level: %d F1 macro: %.4f F1 micro: %.4f\" % (level, macro, micro))\n",
    "    f1_macro, f1_micro, f1_each = model.evaluate(\"test\", correction=True)\n",
    "    print(\"F1 macro: %.4f F1 micro: %.4f\" % (f1_macro, f1_micro))\n",
    "    for level, (macro, micro) in enumerate(f1_each):\n",
    "        print(\"Level: %d F1 macro: %.4f F1 micro: %.4f\" % (level, macro, micro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chanatip/miniconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/chanatip/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
